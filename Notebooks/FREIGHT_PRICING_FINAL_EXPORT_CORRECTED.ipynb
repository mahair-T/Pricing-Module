{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸšš Freight Pricing Model Export - Final Production\n",
    "\n",
    "This notebook trains and exports all pricing models for the Freight Pricing Tool.\n",
    "\n",
    "## Model Architecture\n",
    "\n",
    "**Pricing Cascade (in order of priority):**\n",
    "1. **Recency Model** - Median of loads within last 90 days (if available)\n",
    "2. **Index + Shrinkage Model** - For lanes WITH historical data (uses market index + shrinkage estimation)\n",
    "3. **Province Blend 0.7 + LAD Model** - For lanes with NO historical data (70% province-level CPK + 30% city decomposition, with LAD calibration)\n",
    "\n",
    "## Key Improvements in This Version\n",
    "- **Province-based regions** (13 provinces) instead of 5 custom freight regions\n",
    "- **LAD (L1) calibration** applied to Blend model predictions to minimize MAE\n",
    "- **Comprehensive validation** for each model component\n",
    "\n",
    "## Output Files (to `final_models/`)\n",
    "| File | Description |\n",
    "|------|-------------|\n",
    "| `reference_data.csv` | Historical trip data for KNN lookups |\n",
    "| `config.pkl` | Configuration and distance lookups |\n",
    "| `rare_lane_models.pkl` | Index + Shrinkage model (lanes WITH history) |\n",
    "| `new_lane_model_blend.pkl` | Province Blend 0.7 + LAD model (NEW lanes) |\n",
    "| `distance_matrix.pkl` | City-to-city distance lookup |\n",
    "| `city_normalization_with_regions.csv` | City normalization and province mapping |\n",
    "| `carrier_model.json` | CatBoost model (optional) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# âš™ï¸ GLOBAL CONFIGURATION\n",
    "\n",
    "Edit these settings before running the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Global configuration loaded\n",
      "ğŸ“ Export directory: final_models/\n"
     ]
    }
   ],
   "source": [
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘                         FILE PATHS                                   â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "# Main trip data file (required)\n",
    "TRIP_DATA_FILE = 'Pfull5.csv'\n",
    "\n",
    "# Supporting files\n",
    "BACKHAUL_FILE = 'Backhaul Prob.csv'\n",
    "IMBALANCE_FILE = 'route imbalance.csv'\n",
    "DISTANCE_FILE = '/Users/maherelghor/Documents/All Lane Pricing - Distances.csv'\n",
    "\n",
    "# City regions file - use region_test.csv which has both 'region' (5 regions) and 'province' (13 provinces)\n",
    "CITY_REGIONS_FILE = '/Users/maherelghor/Documents/city_normalization_with_regions.csv'\n",
    "\n",
    "# Output directory\n",
    "EXPORT_DIR = 'final_models'\n",
    "\n",
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘                      DATA FILTERS                                    â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "ENTITY_MAPPING = 'Domestic'\n",
    "VEHICLE_TYPES = ['ØªØ±ÙŠÙ„Ø§ ÙØ±Ø´', 'ØªØ±ÙŠÙ„Ø§ Ø³ØªØ§Ø¦Ø±']  # Flatbed and Curtain trailers\n",
    "MIN_CARRIER_PRICE = 50\n",
    "MAX_CARRIER_PRICE = 100000\n",
    "MIN_DISTANCE = 50  # km\n",
    "MIN_CPK = 0.5  # SAR/km\n",
    "MAX_CPK = 5.0  # SAR/km\n",
    "\n",
    "print(\"âœ… Global configuration loaded\")\n",
    "print(f\"ğŸ“ Export directory: {EXPORT_DIR}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# ğŸ“š IMPORTS & SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Libraries loaded\n",
      "ğŸ“ Export directory: /Users/maherelghor/Downloads/final_models\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import shutil\n",
    "import warnings\n",
    "from scipy.optimize import minimize\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Create export directory\n",
    "os.makedirs(EXPORT_DIR, exist_ok=True)\n",
    "\n",
    "print(\"âœ… Libraries loaded\")\n",
    "print(f\"ğŸ“ Export directory: {os.path.abspath(EXPORT_DIR)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# ğŸ“ LOAD & PREPARE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading trip data...\n",
      "   Raw: 95,634 rows\n",
      "   Date range: 2020-03-06 13:00:00 to 2026-01-21 08:00:00\n"
     ]
    }
   ],
   "source": [
    "# Load main trip data\n",
    "print(\"Loading trip data...\")\n",
    "df_raw = pd.read_csv(TRIP_DATA_FILE)\n",
    "print(f\"   Raw: {len(df_raw):,} rows\")\n",
    "\n",
    "# Clean numeric columns\n",
    "for col in ['distance', 'total_carrier_price', 'total_shipper_price', 'weight', 'cost']:\n",
    "    if col in df_raw.columns:\n",
    "        df_raw[col] = pd.to_numeric(\n",
    "            df_raw[col].astype(str).str.replace(',', ''), \n",
    "            errors='coerce'\n",
    "        )\n",
    "\n",
    "# Handle different column names for cost\n",
    "if 'cost' in df_raw.columns and 'total_carrier_price' not in df_raw.columns:\n",
    "    df_raw['total_carrier_price'] = df_raw['cost']\n",
    "\n",
    "# Parse dates\n",
    "df_raw['pickup_date'] = pd.to_datetime(df_raw['pickup_date'], format='mixed', errors='coerce')\n",
    "\n",
    "print(f\"   Date range: {df_raw['pickup_date'].min()} to {df_raw['pickup_date'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading supporting files...\n",
      "   âœ… Backhaul: 24 cities\n",
      "   âœ… Imbalance: 289 lanes\n",
      "   âœ… City-to-province mappings (13 provinces): 527 cities\n",
      "   âœ… Unique provinces: 13\n",
      "   âœ… City-to-region mappings (5 regions): 528 cities\n",
      "   âœ… Unique regions: 5\n",
      "   ğŸ“‹ Provinces: ['Al Bahah Province', 'Al Jouf Province', 'Asir Province', 'Eastern Province', 'Hail Province', 'Jazan Province', 'Madinah Province', 'Makkah Province', 'Najran Province', 'Northern Borders Province', 'Qassim Province', 'Riyadh Province', 'Tabuk Province']\n",
      "   ğŸ“‹ Regions: ['Central', 'Eastern', 'Northern', 'Southern', 'Western']\n",
      "   âœ… Distances: 4,492 city pairs\n"
     ]
    }
   ],
   "source": [
    "# Load supporting files\n",
    "print(\"\\nLoading supporting files...\")\n",
    "\n",
    "# Backhaul data\n",
    "try:\n",
    "    backhaul_df = pd.read_csv(BACKHAUL_FILE)\n",
    "    print(f\"   âœ… Backhaul: {len(backhaul_df)} cities\")\n",
    "except FileNotFoundError:\n",
    "    backhaul_df = pd.DataFrame(columns=['destination_city', 'backhaul_probability_pct'])\n",
    "    print(\"   âš ï¸ Backhaul file not found - skipping\")\n",
    "\n",
    "# Imbalance data\n",
    "try:\n",
    "    imbalance_df = pd.read_csv(IMBALANCE_FILE)\n",
    "    imbalance_df['load_count'] = pd.to_numeric(\n",
    "        imbalance_df['load_count'].astype(str).str.replace(',', ''), \n",
    "        errors='coerce'\n",
    "    )\n",
    "    imbalance_df['lane'] = imbalance_df['pickup_city'] + ' â†’ ' + imbalance_df['destination_city']\n",
    "    print(f\"   âœ… Imbalance: {len(imbalance_df)} lanes\")\n",
    "except FileNotFoundError:\n",
    "    imbalance_df = pd.DataFrame(columns=['lane', 'load_count', 'load_imbalance'])\n",
    "    print(\"   âš ï¸ Imbalance file not found - skipping\")\n",
    "\n",
    "# City regions/provinces - region_test.csv has both 'region' (5 regions) and 'province' (13 provinces)\n",
    "try:\n",
    "    city_regions_df = pd.read_csv(CITY_REGIONS_FILE)\n",
    "    \n",
    "    # Create PROVINCE mapping (13 provinces) - for Blend model\n",
    "    # Uses the 'province' column from region_test.csv\n",
    "    city_to_province = dict(zip(city_regions_df['variant'], city_regions_df['province']))\n",
    "    city_to_province.update(dict(zip(city_regions_df['canonical'], city_regions_df['province'])))\n",
    "    # Remove None/NaN values\n",
    "    city_to_province = {k: v for k, v in city_to_province.items() if pd.notna(v) and v != ''}\n",
    "    \n",
    "    # Create REGION mapping (5 regions) - for Index+Shrinkage fallback\n",
    "    # Uses the 'region' column from region_test.csv\n",
    "    city_to_region = dict(zip(city_regions_df['variant'], city_regions_df['region']))\n",
    "    city_to_region.update(dict(zip(city_regions_df['canonical'], city_regions_df['region'])))\n",
    "    # Remove None/NaN values\n",
    "    city_to_region = {k: v for k, v in city_to_region.items() if pd.notna(v) and v != ''}\n",
    "    \n",
    "    print(f\"   âœ… City-to-province mappings (13 provinces): {len(city_to_province)} cities\")\n",
    "    print(f\"   âœ… Unique provinces: {len(set(city_to_province.values()))}\")\n",
    "    print(f\"   âœ… City-to-region mappings (5 regions): {len(city_to_region)} cities\")\n",
    "    print(f\"   âœ… Unique regions: {len(set(city_to_region.values()))}\")\n",
    "    \n",
    "    # Print the unique provinces and regions for verification\n",
    "    print(f\"   ğŸ“‹ Provinces: {sorted(set(city_to_province.values()))}\")\n",
    "    print(f\"   ğŸ“‹ Regions: {sorted(set(city_to_region.values()))}\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    city_to_province = {}\n",
    "    city_to_region = {}\n",
    "    print(\"   âš ï¸ City regions file not found\")\n",
    "\n",
    "# Hardcoded distances\n",
    "try:\n",
    "    dist_df = pd.read_csv(DISTANCE_FILE)\n",
    "    if len(dist_df.columns) >= 3:\n",
    "        dist_df.columns = ['pickup_city', 'destination_city', 'distance_km'] + list(dist_df.columns[3:])\n",
    "    dist_df['distance_km'] = pd.to_numeric(dist_df['distance_km'], errors='coerce')\n",
    "    hardcoded_distances = {\n",
    "        (row['pickup_city'], row['destination_city']): row['distance_km']\n",
    "        for _, row in dist_df.iterrows()\n",
    "        if pd.notna(row['distance_km']) and row['distance_km'] > 0\n",
    "    }\n",
    "    print(f\"   âœ… Distances: {len(hardcoded_distances):,} city pairs\")\n",
    "except FileNotFoundError:\n",
    "    hardcoded_distances = {}\n",
    "    print(\"   âš ï¸ Distance file not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering data...\n",
      "   After entity/vehicle filter: 84,052\n",
      "   After CPK filter: 80,524\n",
      "\n",
      "âœ… Clean data: 80,524 trips, 299 unique lanes\n",
      "   Date range: 2020-03-06 to 2026-01-20\n",
      "   Province coverage: 100.0% pickup, 99.2% dest\n"
     ]
    }
   ],
   "source": [
    "# Apply filters and clean data\n",
    "print(\"Filtering data...\")\n",
    "\n",
    "df = df_raw[\n",
    "    (df_raw['entity_mapping'] == ENTITY_MAPPING) &\n",
    "    (df_raw['vehicle_type'].isin(VEHICLE_TYPES)) &\n",
    "    (df_raw['total_carrier_price'] >= MIN_CARRIER_PRICE) &\n",
    "    (df_raw['total_carrier_price'] <= MAX_CARRIER_PRICE) &\n",
    "    (df_raw['distance'] >= MIN_DISTANCE) &\n",
    "    (df_raw['pickup_date'].notna())\n",
    "].copy()\n",
    "\n",
    "print(f\"   After entity/vehicle filter: {len(df):,}\")\n",
    "\n",
    "# Calculate CPK and filter outliers\n",
    "df['cost_per_km'] = df['total_carrier_price'] / df['distance']\n",
    "df = df[\n",
    "    (df['cost_per_km'] >= MIN_CPK) & \n",
    "    (df['cost_per_km'] <= MAX_CPK)\n",
    "].copy()\n",
    "\n",
    "print(f\"   After CPK filter: {len(df):,}\")\n",
    "\n",
    "# Create derived features\n",
    "df['lane'] = df['pickup_city'] + ' â†’ ' + df['destination_city']\n",
    "df['date'] = df['pickup_date'].dt.date\n",
    "\n",
    "# Calculate days_ago from most recent date\n",
    "most_recent = df['pickup_date'].max()\n",
    "df['days_ago'] = (most_recent - df['pickup_date']).dt.days\n",
    "\n",
    "# Fill missing values\n",
    "for col in ['commodity', 'vehicle_type', 'pickup_city', 'destination_city']:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].fillna('Unknown')\n",
    "\n",
    "if 'weight' in df.columns:\n",
    "    df['weight'] = df['weight'].fillna(df['weight'].median())\n",
    "else:\n",
    "    df['weight'] = 25.0\n",
    "\n",
    "if 'container' in df.columns:\n",
    "    df['container'] = df['container'].fillna(0).astype(int)\n",
    "else:\n",
    "    df['container'] = 0\n",
    "\n",
    "if 'leg_name' in df.columns:\n",
    "    df['is_multistop'] = (\n",
    "        (df['leg_name'] == 'Trip') |\n",
    "        ((df['pickup_city'] == df['destination_city']) & (df['distance'] > 100))\n",
    "    ).astype(int)\n",
    "else:\n",
    "    df['is_multistop'] = 0\n",
    "\n",
    "# Map provinces\n",
    "df['pickup_province'] = df['pickup_city'].map(city_to_province)\n",
    "df['dest_province'] = df['destination_city'].map(city_to_province)\n",
    "\n",
    "print(f\"\\nâœ… Clean data: {len(df):,} trips, {df['lane'].nunique()} unique lanes\")\n",
    "print(f\"   Date range: {df['date'].min()} to {df['date'].max()}\")\n",
    "print(f\"   Province coverage: {df['pickup_province'].notna().mean()*100:.1f}% pickup, {df['dest_province'].notna().mean()*100:.1f}% dest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging supporting data...\n",
      "   âœ… Backhaul data merged\n",
      "   âœ… Imbalance data merged\n",
      "\n",
      "âœ… Final dataset: 84,948 rows\n"
     ]
    }
   ],
   "source": [
    "# Merge supporting data\n",
    "print(\"Merging supporting data...\")\n",
    "\n",
    "# Backhaul\n",
    "if 'destination_city' in backhaul_df.columns and 'backhaul_probability_pct' in backhaul_df.columns:\n",
    "    backhaul_merge = backhaul_df[['destination_city', 'backhaul_probability_pct']].copy()\n",
    "    backhaul_merge.columns = ['destination_city', 'dest_backhaul_prob']\n",
    "    df = df.merge(backhaul_merge, on='destination_city', how='left')\n",
    "    df['dest_backhaul_prob'] = df['dest_backhaul_prob'].fillna(20.0)\n",
    "    print(f\"   âœ… Backhaul data merged\")\n",
    "else:\n",
    "    df['dest_backhaul_prob'] = 20.0\n",
    "\n",
    "# Imbalance\n",
    "if 'lane' in imbalance_df.columns:\n",
    "    imb_merge = imbalance_df[['lane', 'load_count', 'load_imbalance']].copy()\n",
    "    imb_merge.columns = ['lane', 'lane_load_count', 'lane_imbalance']\n",
    "    df = df.merge(imb_merge, on='lane', how='left')\n",
    "    df['lane_load_count'] = df['lane_load_count'].fillna(0)\n",
    "    df['lane_imbalance'] = df['lane_imbalance'].fillna(1.0)\n",
    "    print(f\"   âœ… Imbalance data merged\")\n",
    "else:\n",
    "    df['lane_load_count'] = 0\n",
    "    df['lane_imbalance'] = 1.0\n",
    "\n",
    "print(f\"\\nâœ… Final dataset: {len(df):,} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# ğŸ“Š MODEL 1: MARKET INDEX\n",
    "\n",
    "Builds a market-wide CPK index from benchmark (high-volume) lanes.\n",
    "Used by both Index+Shrinkage and Blend models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Market Index Parameters:\n",
      "   Benchmark lanes: 5\n",
      "   Smoothing window: 7 days\n"
     ]
    }
   ],
   "source": [
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘                   MARKET INDEX PARAMETERS                            â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "N_BENCHMARK_LANES = 5       # Number of top lanes to use for index\n",
    "INDEX_SMOOTHING_DAYS = 7    # Rolling average window for smoothing\n",
    "INDEX_LOOKBACK_DAYS = 365   # 1 Year Lookback\n",
    "\n",
    "print(\"Market Index Parameters:\")\n",
    "print(f\"   Benchmark lanes: {N_BENCHMARK_LANES}\")\n",
    "print(f\"   Smoothing window: {INDEX_SMOOTHING_DAYS} days\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Benchmark Lanes (by volume):\n",
      "   1. Ø¬Ø¯Ø© â†’ Ø¬Ø¯Ø©: 22,195 trips\n",
      "   2. Ø§Ù„Ø¬Ø¨ÙŠÙ„ â†’ Ø¬Ø¯Ø©: 9,939 trips\n",
      "   3. Ø§Ù„Ø¯Ù…Ø§Ù… â†’ Ø¬Ø¯Ø©: 5,643 trips\n",
      "   4. Ø¬Ø¯Ø© â†’ Ø§Ù„Ø±ÙŠØ§Ø¶: 5,227 trips\n",
      "   5. Ø§Ù„Ø±ÙŠØ§Ø¶ â†’ Ø¬Ø¯Ø©: 5,169 trips\n",
      "\n",
      "âœ… Market Index Built\n",
      "   Current index: 1.7109 SAR/km\n",
      "   Index range: 1.155 - 2.537\n"
     ]
    }
   ],
   "source": [
    "# Identify top benchmark lanes\n",
    "lane_counts = df['lane'].value_counts()\n",
    "benchmark_lanes = lane_counts.head(N_BENCHMARK_LANES).index.tolist()\n",
    "\n",
    "print(\"ğŸ“Š Benchmark Lanes (by volume):\")\n",
    "for i, lane in enumerate(benchmark_lanes, 1):\n",
    "    count = lane_counts[lane]\n",
    "    print(f\"   {i}. {lane}: {count:,} trips\")\n",
    "\n",
    "# Build daily market index\n",
    "# Filter for benchmark lanes AND recency (last 365 days)\n",
    "benchmark_df = df[\n",
    "    (df['lane'].isin(benchmark_lanes)) & \n",
    "    (df['days_ago'] <= INDEX_LOOKBACK_DAYS)\n",
    "]\n",
    "daily_index = benchmark_df.groupby('date')['cost_per_km'].median().reset_index()\n",
    "daily_index.columns = ['date', 'index_value']\n",
    "daily_index = daily_index.sort_values('date')\n",
    "\n",
    "# Smooth with rolling average\n",
    "daily_index['index_smoothed'] = daily_index['index_value'].rolling(\n",
    "    INDEX_SMOOTHING_DAYS, min_periods=1, center=True\n",
    ").mean()\n",
    "\n",
    "current_index = float(daily_index['index_smoothed'].iloc[-1])\n",
    "\n",
    "print(f\"\\nâœ… Market Index Built\")\n",
    "print(f\"   Current index: {current_index:.4f} SAR/km\")\n",
    "print(f\"   Index range: {daily_index['index_smoothed'].min():.3f} - {daily_index['index_smoothed'].max():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Lane multipliers: 299\n",
      "\n",
      "Sample multipliers:\n",
      "   Ø¬Ø¯Ø© â†’ Ø¬Ø¯Ø©: 0.961x\n",
      "   Ø§Ù„Ø¬Ø¨ÙŠÙ„ â†’ Ø¬Ø¯Ø©: 1.023x\n",
      "   Ø§Ù„Ø¯Ù…Ø§Ù… â†’ Ø¬Ø¯Ø©: 1.071x\n"
     ]
    }
   ],
   "source": [
    "# Calculate lane multipliers (lane CPK relative to index)\n",
    "df_indexed = df.merge(daily_index[['date', 'index_smoothed']], on='date', how='left')\n",
    "df_indexed['index_smoothed'] = df_indexed['index_smoothed'].ffill().bfill()\n",
    "df_indexed['multiplier'] = df_indexed['cost_per_km'] / df_indexed['index_smoothed']\n",
    "\n",
    "lane_multipliers = df_indexed.groupby('lane')['multiplier'].median().to_dict()\n",
    "\n",
    "print(f\"âœ… Lane multipliers: {len(lane_multipliers)}\")\n",
    "print(\"\\nSample multipliers:\")\n",
    "for lane in benchmark_lanes[:3]:\n",
    "    mult = lane_multipliers.get(lane, 1.0)\n",
    "    print(f\"   {lane}: {mult:.3f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# ğŸ¯ MODEL 2: INDEX + SHRINKAGE (rare_lane_models.pkl)\n",
    "\n",
    "For lanes **WITH** some historical data. Combines:\n",
    "- **Index component**: Lane multiplier Ã— current market index\n",
    "- **Shrinkage component**: Bayesian estimate that pulls sparse lanes toward city priors\n",
    "\n",
    "Final prediction = average of Index and Shrinkage components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index + Shrinkage Parameters:\n",
      "   Prior strength (k): 10\n",
      "   Interpretation: Lane with n trips gets weight = n/(n+10)\n"
     ]
    }
   ],
   "source": [
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘               INDEX + SHRINKAGE PARAMETERS                           â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "SHRINKAGE_K = 10    # Prior strength: higher = more shrinkage to city average\n",
    "                    # At k=10, a lane needs ~10 trips to rely 50% on its own mean\n",
    "\n",
    "print(\"Index + Shrinkage Parameters:\")\n",
    "print(f\"   Prior strength (k): {SHRINKAGE_K}\")\n",
    "print(f\"   Interpretation: Lane with n trips gets weight = n/(n+{SHRINKAGE_K})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Shrinkage Model...\n",
      "\n",
      "âœ… Shrinkage Model Built\n",
      "   Global mean: 1.863 SAR/km\n",
      "   Pickup priors: 35 cities\n",
      "   Destination priors: 62 cities\n",
      "   Lane stats: 299 lanes\n"
     ]
    }
   ],
   "source": [
    "# Build Shrinkage Model components\n",
    "print(\"Building Shrinkage Model...\")\n",
    "\n",
    "# Global mean CPK\n",
    "global_mean = float(df['cost_per_km'].mean())\n",
    "\n",
    "# City priors (average CPK for each city)\n",
    "pickup_priors = df.groupby('pickup_city')['cost_per_km'].mean().to_dict()\n",
    "dest_priors = df.groupby('destination_city')['cost_per_km'].mean().to_dict()\n",
    "\n",
    "# Lane statistics for shrinkage\n",
    "lane_stats = df.groupby('lane').agg(\n",
    "    lane_mean=('cost_per_km', 'mean'),\n",
    "    lane_n=('cost_per_km', 'count'),\n",
    "    pickup_city=('pickup_city', 'first'),\n",
    "    destination_city=('destination_city', 'first')\n",
    ").to_dict('index')\n",
    "\n",
    "print(f\"\\nâœ… Shrinkage Model Built\")\n",
    "print(f\"   Global mean: {global_mean:.3f} SAR/km\")\n",
    "print(f\"   Pickup priors: {len(pickup_priors)} cities\")\n",
    "print(f\"   Destination priors: {len(dest_priors)} cities\")\n",
    "print(f\"   Lane stats: {len(lane_stats)} lanes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building 5-Region Model (for Index+Shrinkage fallback)...\n",
      "\n",
      "âœ… 5-Region Model Built\n",
      "   Regional corridors: 24\n",
      "   City-to-region mappings: 528\n",
      "   Region coverage: 99.8% pickup, 94.3% dest\n"
     ]
    }
   ],
   "source": [
    "# Build regional fallback (for Index+Shrinkage model, uses 5 regions)\n",
    "print(\"Building 5-Region Model (for Index+Shrinkage fallback)...\")\n",
    "\n",
    "# city_to_region already contains the 5 macro-regions (Eastern, Western, Central, Northern, Southern)\n",
    "# directly loaded from the 'region' column of region_test.csv\n",
    "city_to_region_5 = city_to_region.copy()  # Already has 5 regions from earlier cell\n",
    "\n",
    "# Map regions to dataframe\n",
    "df['pickup_region_5'] = df['pickup_city'].map(city_to_region_5)\n",
    "df['dest_region_5'] = df['destination_city'].map(city_to_region_5)\n",
    "\n",
    "# Build regional CPK matrix (5 regions)\n",
    "df_regional = df[df['pickup_region_5'].notna() & df['dest_region_5'].notna()]\n",
    "regional_cpk_5 = df_regional.groupby(\n",
    "    ['pickup_region_5', 'dest_region_5']\n",
    ")['cost_per_km'].median().to_dict()\n",
    "\n",
    "print(f\"\\nâœ… 5-Region Model Built\")\n",
    "print(f\"   Regional corridors: {len(regional_cpk_5)}\")\n",
    "print(f\"   City-to-region mappings: {len(city_to_region_5)}\")\n",
    "print(f\"   Region coverage: {df['pickup_region_5'].notna().mean()*100:.1f}% pickup, {df['dest_region_5'].notna().mean()*100:.1f}% dest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Distance Coverage:\n",
      "   Historical: 312 pairs\n",
      "   Hardcoded:  4492 pairs\n",
      "   Overlap: 3 pairs\n",
      "   Total unique: 4801 pairs\n"
     ]
    }
   ],
   "source": [
    "# Build distance lookups\n",
    "historical_distances = df.groupby(\n",
    "    ['pickup_city', 'destination_city']\n",
    ")['distance'].median().to_dict()\n",
    "\n",
    "print(f\"ğŸ“Š Distance Coverage:\")\n",
    "print(f\"   Historical: {len(historical_distances)} pairs\")\n",
    "print(f\"   Hardcoded:  {len(hardcoded_distances)} pairs\")\n",
    "\n",
    "historical_keys = set(historical_distances.keys())\n",
    "hardcoded_keys = set(hardcoded_distances.keys())\n",
    "print(f\"   Overlap: {len(historical_keys & hardcoded_keys)} pairs\")\n",
    "print(f\"   Total unique: {len(historical_keys | hardcoded_keys)} pairs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Saved: final_models/rare_lane_models.pkl\n"
     ]
    }
   ],
   "source": [
    "# Package rare_lane_models.pkl\n",
    "rare_lane_artifacts = {\n",
    "    # Distance lookups\n",
    "    'historical_distances': historical_distances,\n",
    "    'hardcoded_distances': hardcoded_distances,\n",
    "    \n",
    "    # Index model\n",
    "    'current_index': current_index,\n",
    "    'lane_multipliers': lane_multipliers,\n",
    "    'benchmark_lanes': benchmark_lanes,\n",
    "    \n",
    "    # Shrinkage model\n",
    "    'global_mean': global_mean,\n",
    "    'k_prior_strength': SHRINKAGE_K,\n",
    "    'pickup_priors': pickup_priors,\n",
    "    'dest_priors': dest_priors,\n",
    "    'lane_stats': lane_stats,\n",
    "    \n",
    "    # Regional fallback (5 regions)\n",
    "    'city_to_region': city_to_region_5,\n",
    "    'regional_cpk': regional_cpk_5,\n",
    "    \n",
    "    # Metadata\n",
    "    'model_date': str(df['date'].max()),\n",
    "    'training_trips': len(df),\n",
    "    'training_lanes': df['lane'].nunique()\n",
    "}\n",
    "\n",
    "with open(f'{EXPORT_DIR}/rare_lane_models.pkl', 'wb') as f:\n",
    "    pickle.dump(rare_lane_artifacts, f)\n",
    "\n",
    "print(f\"\\nâœ… Saved: {EXPORT_DIR}/rare_lane_models.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ§ª VALIDATION: Index + Shrinkage Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… RareLanePredictor class defined\n"
     ]
    }
   ],
   "source": [
    "# Define predictor class for validation\n",
    "class RareLanePredictor:\n",
    "    def __init__(self, artifacts):\n",
    "        self.current_index = artifacts['current_index']\n",
    "        self.lane_multipliers = artifacts['lane_multipliers']\n",
    "        self.global_mean = artifacts['global_mean']\n",
    "        self.k = artifacts['k_prior_strength']\n",
    "        self.pickup_priors = artifacts['pickup_priors']\n",
    "        self.dest_priors = artifacts['dest_priors']\n",
    "        self.lane_stats = artifacts['lane_stats']\n",
    "        self.historical_distances = artifacts['historical_distances']\n",
    "        self.hardcoded_distances = artifacts['hardcoded_distances']\n",
    "    \n",
    "    def get_distance(self, pickup, dest):\n",
    "        key = (pickup, dest)\n",
    "        if key in self.historical_distances:\n",
    "            return self.historical_distances[key], 'historical'\n",
    "        if key in self.hardcoded_distances:\n",
    "            return self.hardcoded_distances[key], 'hardcoded'\n",
    "        return None, 'not_found'\n",
    "    \n",
    "    def predict(self, pickup, dest):\n",
    "        lane = f\"{pickup} â†’ {dest}\"\n",
    "        \n",
    "        # Index prediction\n",
    "        idx_pred = None\n",
    "        if lane in self.lane_multipliers:\n",
    "            idx_pred = self.current_index * self.lane_multipliers[lane]\n",
    "        \n",
    "        # Shrinkage prediction\n",
    "        p_prior = self.pickup_priors.get(pickup, self.global_mean)\n",
    "        d_prior = self.dest_priors.get(dest, self.global_mean)\n",
    "        city_prior = (p_prior + d_prior) / 2\n",
    "        \n",
    "        if lane in self.lane_stats:\n",
    "            stats = self.lane_stats[lane]\n",
    "            lam = stats['lane_n'] / (stats['lane_n'] + self.k)\n",
    "            shrink_pred = lam * stats['lane_mean'] + (1 - lam) * city_prior\n",
    "            n_trips = stats['lane_n']\n",
    "        else:\n",
    "            shrink_pred = city_prior\n",
    "            n_trips = 0\n",
    "        \n",
    "        # Combine\n",
    "        if idx_pred is not None:\n",
    "            final = (idx_pred + shrink_pred) / 2\n",
    "            method = 'Index + Shrinkage'\n",
    "        else:\n",
    "            final = shrink_pred\n",
    "            method = 'Shrinkage only'\n",
    "        \n",
    "        confidence = 'High' if n_trips >= 20 else ('Medium' if n_trips >= 5 else 'Low')\n",
    "        \n",
    "        return {\n",
    "            'lane': lane, 'predicted_cpk': final, 'method': method,\n",
    "            'confidence': confidence, 'n_trips': n_trips\n",
    "        }\n",
    "\n",
    "print(\"âœ… RareLanePredictor class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "VALIDATION: Index + Shrinkage Model\n",
      "======================================================================\n",
      "\n",
      "High volume: Ø¬Ø¯Ø© â†’ Ø§Ù„Ø±ÙŠØ§Ø¶\n",
      "   CPK: 1.556 SAR/km\n",
      "   Method: Index + Shrinkage | Confidence: High (n=5227)\n",
      "   Distance: 949 km (historical) | Total: 1477 SAR\n",
      "\n",
      "High volume: Ø§Ù„Ø¬Ø¨ÙŠÙ„ â†’ Ø¬Ø¯Ø©\n",
      "   CPK: 1.775 SAR/km\n",
      "   Method: Index + Shrinkage | Confidence: High (n=9939)\n",
      "   Distance: 1412 km (historical) | Total: 2507 SAR\n",
      "\n",
      "Medium volume: ÙŠÙ†Ø¨Ø¹ â†’ ØªØ¨ÙˆÙƒ\n",
      "   CPK: 2.112 SAR/km\n",
      "   Method: Index + Shrinkage | Confidence: High (n=87)\n",
      "   Distance: 744 km (historical) | Total: 1571 SAR\n",
      "\n",
      "Sparse lane: Ø³Ø¯ÙŠØ± â†’ Ø¬Ø§Ø²Ø§Ù†\n",
      "   CPK: 1.733 SAR/km\n",
      "   Method: Index + Shrinkage | Confidence: Medium (n=8)\n",
      "   Distance: 1077 km (historical) | Total: 1865 SAR\n"
     ]
    }
   ],
   "source": [
    "# Test the predictor\n",
    "print(\"=\" * 70)\n",
    "print(\"VALIDATION: Index + Shrinkage Model\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "predictor = RareLanePredictor(rare_lane_artifacts)\n",
    "\n",
    "test_cases = [\n",
    "    ('Ø¬Ø¯Ø©', 'Ø§Ù„Ø±ÙŠØ§Ø¶', 'High volume'),\n",
    "    ('Ø§Ù„Ø¬Ø¨ÙŠÙ„', 'Ø¬Ø¯Ø©', 'High volume'),\n",
    "    ('ÙŠÙ†Ø¨Ø¹', 'ØªØ¨ÙˆÙƒ', 'Medium volume'),\n",
    "    ('Ø³Ø¯ÙŠØ±', 'Ø¬Ø§Ø²Ø§Ù†', 'Sparse lane'),\n",
    "]\n",
    "\n",
    "for pickup, dest, desc in test_cases:\n",
    "    result = predictor.predict(pickup, dest)\n",
    "    dist, src = predictor.get_distance(pickup, dest)\n",
    "    print(f\"\\n{desc}: {result['lane']}\")\n",
    "    print(f\"   CPK: {result['predicted_cpk']:.3f} SAR/km\")\n",
    "    print(f\"   Method: {result['method']} | Confidence: {result['confidence']} (n={result['n_trips']})\")\n",
    "    if dist:\n",
    "        print(f\"   Distance: {dist:.0f} km ({src}) | Total: {result['predicted_cpk']*dist:.0f} SAR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "QUANTITATIVE VALIDATION: Index + Shrinkage\n",
      "======================================================================\n",
      "\n",
      "Holdout: 714 trips (last 30 days)\n",
      "Training: 84,234 trips\n",
      "\n",
      "Index + Shrinkage Model Performance:\n",
      "   MAE:  0.2054 SAR/km\n",
      "   RMSE: 0.2655 SAR/km\n",
      "   Bias: -0.1385 SAR/km\n",
      "   Within Â±0.25: 76.6%\n",
      "   Within Â±0.50: 91.3%\n"
     ]
    }
   ],
   "source": [
    "# Quantitative validation on holdout\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"QUANTITATIVE VALIDATION: Index + Shrinkage\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Use most recent 30 days as holdout\n",
    "holdout = df[df['days_ago'] <= 30].copy()\n",
    "train_for_val = df[df['days_ago'] > 30].copy()\n",
    "\n",
    "print(f\"\\nHoldout: {len(holdout):,} trips (last 30 days)\")\n",
    "print(f\"Training: {len(train_for_val):,} trips\")\n",
    "\n",
    "# Predict on holdout\n",
    "predictions = []\n",
    "actuals = []\n",
    "\n",
    "for _, row in holdout.iterrows():\n",
    "    result = predictor.predict(row['pickup_city'], row['destination_city'])\n",
    "    predictions.append(result['predicted_cpk'])\n",
    "    actuals.append(row['cost_per_km'])\n",
    "\n",
    "predictions = np.array(predictions)\n",
    "actuals = np.array(actuals)\n",
    "residuals = predictions - actuals\n",
    "\n",
    "print(f\"\\nIndex + Shrinkage Model Performance:\")\n",
    "print(f\"   MAE:  {np.mean(np.abs(residuals)):.4f} SAR/km\")\n",
    "print(f\"   RMSE: {np.sqrt(np.mean(residuals**2)):.4f} SAR/km\")\n",
    "print(f\"   Bias: {np.mean(residuals):.4f} SAR/km\")\n",
    "print(f\"   Within Â±0.25: {(np.abs(residuals) <= 0.25).mean()*100:.1f}%\")\n",
    "print(f\"   Within Â±0.50: {(np.abs(residuals) <= 0.50).mean()*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# ğŸ”® MODEL 3: PROVINCE BLEND 0.7 + LAD (new_lane_model_blend.pkl)\n",
    "\n",
    "For lanes with **NO** historical data. Uses:\n",
    "- **Province-level CPK** (13 official Saudi provinces from `region_test.csv` 'province' column)\n",
    "- **City decomposition** (pickup_mult Ã— dest_mult Ã— index)\n",
    "- **LAD (L1) calibration** to minimize MAE on predictions\n",
    "\n",
    "**Key difference from Index+Shrinkage:**\n",
    "- Province Blend uses **13 provinces** (e.g., \"Riyadh Province\", \"Eastern Province\", \"Makkah Province\")\n",
    "- Index+Shrinkage fallback uses **5 regions** (Eastern, Western, Central, Northern, Southern)\n",
    "\n",
    "**Formula:**\n",
    "```\n",
    "raw_cpk = 0.7 Ã— Province_CPK + 0.3 Ã— City_Decomposition\n",
    "calibrated_cpk = LAD_slope Ã— raw_cpk + LAD_intercept\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Province Blend + LAD Parameters:\n",
      "   Province weight: 0.7 (city decomp: 0.30000000000000004)\n",
      "   LAD calibration: Enabled\n",
      "   Holdout for fitting: 30 days\n"
     ]
    }
   ],
   "source": [
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘               PROVINCE BLEND + LAD PARAMETERS                        â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "PROVINCE_WEIGHT = 0.7       # Weight for province-level prediction (vs city decomposition)\n",
    "LAD_ENABLED = True          # Enable LAD calibration (minimizes MAE)\n",
    "HOLDOUT_DAYS = 30           # Days to hold out for LAD calibration fitting\n",
    "\n",
    "print(\"Province Blend + LAD Parameters:\")\n",
    "print(f\"   Province weight: {PROVINCE_WEIGHT} (city decomp: {1-PROVINCE_WEIGHT})\")\n",
    "print(f\"   LAD calibration: {'Enabled' if LAD_ENABLED else 'Disabled'}\")\n",
    "print(f\"   Holdout for fitting: {HOLDOUT_DAYS} days\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Province Blend Model...\n",
      "   Unique provinces: 13\n",
      "   Province list: ['Al Bahah Province', 'Al Jouf Province', 'Asir Province', 'Eastern Province', 'Hail Province', 'Jazan Province', 'Madinah Province', 'Makkah Province', 'Najran Province', 'Northern Borders Province', 'Qassim Province', 'Riyadh Province', 'Tabuk Province']\n",
      "   Province corridors: 77\n",
      "   Pickup city multipliers: 35\n",
      "   Dest city multipliers: 62\n"
     ]
    }
   ],
   "source": [
    "# Build Province-level CPK matrix (13 official Saudi provinces)\n",
    "print(\"Building Province Blend Model...\")\n",
    "\n",
    "# Get unique provinces (13 provinces from region_test.csv 'province' column)\n",
    "unique_provinces = set(city_to_province.values()) - {None, '', np.nan}\n",
    "print(f\"   Unique provinces: {len(unique_provinces)}\")\n",
    "print(f\"   Province list: {sorted(unique_provinces)}\")\n",
    "\n",
    "# Build province CPK matrix (uses 13 provinces for more granularity)\n",
    "df_province = df[df['pickup_province'].notna() & df['dest_province'].notna()]\n",
    "province_cpk = df_province.groupby(\n",
    "    ['pickup_province', 'dest_province']\n",
    ")['cost_per_km'].median().to_dict()\n",
    "\n",
    "print(f\"   Province corridors: {len(province_cpk)}\")\n",
    "\n",
    "# City decomposition multipliers (used for city-level component of blend)\n",
    "pickup_city_mult = df_indexed.groupby('pickup_city')['multiplier'].median().to_dict()\n",
    "dest_city_mult = df_indexed.groupby('destination_city')['multiplier'].median().to_dict()\n",
    "\n",
    "print(f\"   Pickup city multipliers: {len(pickup_city_mult)}\")\n",
    "print(f\"   Dest city multipliers: {len(dest_city_mult)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… predict_blend_raw function defined (uses 13 provinces)\n"
     ]
    }
   ],
   "source": [
    "# Define raw blend prediction function (before LAD calibration)\n",
    "def predict_blend_raw(pickup, dest, province_weight=PROVINCE_WEIGHT):\n",
    "    \"\"\"\n",
    "    Predict raw CPK using Province Blend (before LAD calibration).\n",
    "    \n",
    "    Uses 13 Saudi provinces from region_test.csv 'province' column:\n",
    "    - e.g., \"Riyadh Province\", \"Eastern Province\", \"Makkah Province\", etc.\n",
    "    \n",
    "    NOT the 5 macro-regions (Eastern, Western, Central, Northern, Southern)\n",
    "    which are used by Index+Shrinkage fallback.\n",
    "    \"\"\"\n",
    "    # Province prediction (13 provinces)\n",
    "    p_province = city_to_province.get(pickup)\n",
    "    d_province = city_to_province.get(dest)\n",
    "    \n",
    "    province_pred = None\n",
    "    if p_province and d_province:\n",
    "        province_pred = province_cpk.get((p_province, d_province))\n",
    "    \n",
    "    # City decomposition prediction\n",
    "    city_pred = None\n",
    "    p_mult = pickup_city_mult.get(pickup)\n",
    "    d_mult = dest_city_mult.get(dest)\n",
    "    if p_mult and d_mult:\n",
    "        city_pred = current_index * p_mult * d_mult\n",
    "    \n",
    "    # Blend: 70% province + 30% city decomposition\n",
    "    if province_pred is not None and city_pred is not None:\n",
    "        return province_weight * province_pred + (1 - province_weight) * city_pred\n",
    "    elif province_pred is not None:\n",
    "        return province_pred\n",
    "    elif city_pred is not None:\n",
    "        return city_pred\n",
    "    else:\n",
    "        return global_mean  # Ultimate fallback\n",
    "\n",
    "print(\"âœ… predict_blend_raw function defined (uses 13 provinces)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fitting LAD Calibration...\n",
      "   Training samples: 84,948\n",
      "\n",
      "âœ… LAD Calibration Fitted\n",
      "   Formula: P_calibrated = 0.9800 Ã— P_raw + -0.0108\n",
      "   OLS comparison: P_calibrated = 0.8533 Ã— P_raw + 0.2614\n"
     ]
    }
   ],
   "source": [
    "# Fit LAD (L1) calibration\n",
    "print(\"\\nFitting LAD Calibration...\")\n",
    "\n",
    "# Generate predictions on training data\n",
    "blend_preds_train = []\n",
    "actuals_train = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    pred = predict_blend_raw(row['pickup_city'], row['destination_city'])\n",
    "    if pred is not None:\n",
    "        blend_preds_train.append(pred)\n",
    "        actuals_train.append(row['cost_per_km'])\n",
    "\n",
    "blend_preds_train = np.array(blend_preds_train)\n",
    "actuals_train = np.array(actuals_train)\n",
    "\n",
    "print(f\"   Training samples: {len(blend_preds_train):,}\")\n",
    "\n",
    "# Fit LAD regression: actual = m * predicted + c\n",
    "def lad_loss(params, x, y):\n",
    "    m, c = params\n",
    "    return np.sum(np.abs(y - (m * x + c)))\n",
    "\n",
    "# OLS for initial guess\n",
    "ols_coeffs = np.polyfit(blend_preds_train, actuals_train, 1)\n",
    "initial_guess = [ols_coeffs[0], ols_coeffs[1]]\n",
    "\n",
    "# LAD optimization\n",
    "result = minimize(lad_loss, initial_guess, args=(blend_preds_train, actuals_train), method='Nelder-Mead')\n",
    "lad_slope, lad_intercept = result.x\n",
    "\n",
    "print(f\"\\nâœ… LAD Calibration Fitted\")\n",
    "print(f\"   Formula: P_calibrated = {lad_slope:.4f} Ã— P_raw + {lad_intercept:.4f}\")\n",
    "print(f\"   OLS comparison: P_calibrated = {ols_coeffs[0]:.4f} Ã— P_raw + {ols_coeffs[1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "LAD CALIBRATION IMPACT\n",
      "======================================================================\n",
      "\n",
      "Metric               Raw Blend       LAD Calibrated  Improvement    \n",
      "-----------------------------------------------------------------\n",
      "MAE                  0.3260          0.3226          +1.0%\n",
      "Bias                 0.0140          -0.0344         +0.0204\n",
      "RMSE                 0.4812          0.4808          +0.1%\n",
      "Within Â±0.25         51.6           % 52.5           % +1.0pp\n",
      "Within Â±0.50         84.7           % 84.9           % +0.3pp\n"
     ]
    }
   ],
   "source": [
    "# Compare raw vs calibrated performance\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"LAD CALIBRATION IMPACT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "raw_residuals = blend_preds_train - actuals_train\n",
    "calibrated_preds = lad_slope * blend_preds_train + lad_intercept\n",
    "calibrated_residuals = calibrated_preds - actuals_train\n",
    "\n",
    "print(f\"\\n{'Metric':<20} {'Raw Blend':<15} {'LAD Calibrated':<15} {'Improvement':<15}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "raw_mae = np.mean(np.abs(raw_residuals))\n",
    "cal_mae = np.mean(np.abs(calibrated_residuals))\n",
    "print(f\"{'MAE':<20} {raw_mae:<15.4f} {cal_mae:<15.4f} {(raw_mae-cal_mae)/raw_mae*100:>+.1f}%\")\n",
    "\n",
    "raw_bias = np.mean(raw_residuals)\n",
    "cal_bias = np.mean(calibrated_residuals)\n",
    "print(f\"{'Bias':<20} {raw_bias:<15.4f} {cal_bias:<15.4f} {abs(cal_bias)-abs(raw_bias):>+.4f}\")\n",
    "\n",
    "raw_rmse = np.sqrt(np.mean(raw_residuals**2))\n",
    "cal_rmse = np.sqrt(np.mean(calibrated_residuals**2))\n",
    "print(f\"{'RMSE':<20} {raw_rmse:<15.4f} {cal_rmse:<15.4f} {(raw_rmse-cal_rmse)/raw_rmse*100:>+.1f}%\")\n",
    "\n",
    "raw_25 = (np.abs(raw_residuals) <= 0.25).mean() * 100\n",
    "cal_25 = (np.abs(calibrated_residuals) <= 0.25).mean() * 100\n",
    "print(f\"{'Within Â±0.25':<20} {raw_25:<15.1f}% {cal_25:<15.1f}% {cal_25-raw_25:>+.1f}pp\")\n",
    "\n",
    "raw_50 = (np.abs(raw_residuals) <= 0.50).mean() * 100\n",
    "cal_50 = (np.abs(calibrated_residuals) <= 0.50).mean() * 100\n",
    "print(f\"{'Within Â±0.50':<20} {raw_50:<15.1f}% {cal_50:<15.1f}% {cal_50-raw_50:>+.1f}pp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Saved: final_models/new_lane_model_blend.pkl\n",
      "   Province corridors: 77 (13 provinces)\n",
      "   Region corridors: 24 (5 regions)\n",
      "   City-to-province: 527 mappings\n",
      "   City-to-region: 528 mappings\n"
     ]
    }
   ],
   "source": [
    "# Package new_lane_model_blend.pkl\n",
    "blend_artifacts = {\n",
    "    'config': {\n",
    "        'regional_weight': PROVINCE_WEIGHT,  # Keep name for backward compatibility with app\n",
    "        'province_weight': PROVINCE_WEIGHT,\n",
    "        'lad_enabled': LAD_ENABLED,\n",
    "        'lad_slope': float(lad_slope),\n",
    "        'lad_intercept': float(lad_intercept),\n",
    "        'bias_correction': 0,  # Deprecated - LAD handles this now\n",
    "        'training_date': str(df['date'].max()),\n",
    "        'n_trips': len(df),\n",
    "        'n_lanes': df['lane'].nunique(),\n",
    "        'model_type': 'Province Blend 0.7 + LAD',\n",
    "        'n_provinces': len(set(city_to_province.values())),  # Should be ~13\n",
    "        'n_regions': len(set(city_to_region.values()))  # Should be 5\n",
    "    },\n",
    "    \n",
    "    # Province-level (13 Saudi provinces) - for Blend model's primary grouping\n",
    "    'province_cpk': province_cpk,  # (province, province) -> CPK\n",
    "    'city_to_province': city_to_province,  # city -> province (13 provinces)\n",
    "    \n",
    "    # Region-level (5 regions) - for backward compatibility with app's regional fallback\n",
    "    'regional_cpk': regional_cpk_5,  # (region, region) -> CPK\n",
    "    'city_to_region': city_to_region_5,  # city -> region (5 regions)\n",
    "    \n",
    "    # City decomposition (30% weight in blend)\n",
    "    'current_index': current_index,\n",
    "    'benchmark_lanes': benchmark_lanes,\n",
    "    'pickup_city_mult': pickup_city_mult,\n",
    "    'dest_city_mult': dest_city_mult,\n",
    "    \n",
    "    # LAD calibration\n",
    "    'lad_slope': float(lad_slope),\n",
    "    'lad_intercept': float(lad_intercept),\n",
    "}\n",
    "\n",
    "with open(f'{EXPORT_DIR}/new_lane_model_blend.pkl', 'wb') as f:\n",
    "    pickle.dump(blend_artifacts, f)\n",
    "\n",
    "print(f\"\\nâœ… Saved: {EXPORT_DIR}/new_lane_model_blend.pkl\")\n",
    "print(f\"   Province corridors: {len(province_cpk)} (13 provinces)\")\n",
    "print(f\"   Region corridors: {len(regional_cpk_5)} (5 regions)\")\n",
    "print(f\"   City-to-province: {len(city_to_province)} mappings\")\n",
    "print(f\"   City-to-region: {len(city_to_region_5)} mappings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ§ª VALIDATION: Province Blend + LAD Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… predict_blend_calibrated function defined\n"
     ]
    }
   ],
   "source": [
    "# Define calibrated prediction function\n",
    "def predict_blend_calibrated(pickup, dest, model=blend_artifacts):\n",
    "    \"\"\"\n",
    "    Predict CPK using Province Blend + LAD calibration.\n",
    "    \"\"\"\n",
    "    province_weight = model['config']['province_weight']\n",
    "    lad_slope = model['lad_slope']\n",
    "    lad_intercept = model['lad_intercept']\n",
    "    \n",
    "    # Province prediction\n",
    "    p_province = model['city_to_province'].get(pickup)\n",
    "    d_province = model['city_to_province'].get(dest)\n",
    "    \n",
    "    province_pred = None\n",
    "    if p_province and d_province:\n",
    "        province_pred = model['province_cpk'].get((p_province, d_province))\n",
    "    \n",
    "    # City decomposition\n",
    "    city_pred = None\n",
    "    p_mult = model['pickup_city_mult'].get(pickup)\n",
    "    d_mult = model['dest_city_mult'].get(dest)\n",
    "    if p_mult and d_mult:\n",
    "        city_pred = model['current_index'] * p_mult * d_mult\n",
    "    \n",
    "    # Blend\n",
    "    if province_pred is not None and city_pred is not None:\n",
    "        raw_pred = province_weight * province_pred + (1 - province_weight) * city_pred\n",
    "        method = f'Blend ({province_weight:.0%} Province)'\n",
    "    elif province_pred is not None:\n",
    "        raw_pred = province_pred\n",
    "        method = 'Province only'\n",
    "    elif city_pred is not None:\n",
    "        raw_pred = city_pred\n",
    "        method = 'City decomposition only'\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "    # Apply LAD calibration\n",
    "    calibrated = lad_slope * raw_pred + lad_intercept\n",
    "    \n",
    "    return {\n",
    "        'raw_cpk': raw_pred,\n",
    "        'predicted_cpk': calibrated,\n",
    "        'method': method + ' + LAD',\n",
    "        'province_cpk': province_pred,\n",
    "        'city_decomp_cpk': city_pred,\n",
    "        'pickup_province': p_province,\n",
    "        'dest_province': d_province\n",
    "    }\n",
    "\n",
    "print(\"âœ… predict_blend_calibrated function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "VALIDATION: Province Blend + LAD Model\n",
      "======================================================================\n",
      "\n",
      "Ø§Ù„Ø±ÙŠØ§Ø¶ â†’ Ø¬Ø¯Ø©:\n",
      "   Raw: 1.705 â†’ Calibrated: 1.660 SAR/km\n",
      "   Method: Blend (70% Province) + LAD\n",
      "   Province (Riyadh Provinceâ†’Makkah Province): 1.585\n",
      "   City decomp: 1.985\n",
      "\n",
      "Ø§Ù„Ø¯Ù…Ø§Ù… â†’ Ø§Ù„Ø±ÙŠØ§Ø¶:\n",
      "   Raw: 2.503 â†’ Calibrated: 2.442 SAR/km\n",
      "   Method: Blend (70% Province) + LAD\n",
      "   Province (Eastern Provinceâ†’Riyadh Province): 2.660\n",
      "   City decomp: 2.135\n",
      "\n",
      "Ø¬Ø¯Ø© â†’ Ø§Ù„Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„Ù…Ù†ÙˆØ±Ø©:\n",
      "   Raw: 2.106 â†’ Calibrated: 2.053 SAR/km\n",
      "   Method: Blend (70% Province) + LAD\n",
      "   Province (Makkah Provinceâ†’Madinah Province): 2.075\n",
      "   City decomp: 2.180\n",
      "\n",
      "ØªØ¨ÙˆÙƒ â†’ Ø§Ù„Ø¯Ù…Ø§Ù…:\n",
      "   Raw: 1.719 â†’ Calibrated: 1.674 SAR/km\n",
      "   Method: Blend (70% Province) + LAD\n",
      "   Province (Tabuk Provinceâ†’Eastern Province): 1.592\n",
      "   City decomp: 2.015\n",
      "\n",
      "Ø§Ù„Ø¬Ø¨ÙŠÙ„ â†’ Ø§Ù„Ø±ÙŠØ§Ø¶:\n",
      "   Raw: 2.437 â†’ Calibrated: 2.378 SAR/km\n",
      "   Method: Blend (70% Province) + LAD\n",
      "   Province (Eastern Provinceâ†’Riyadh Province): 2.660\n",
      "   City decomp: 1.918\n"
     ]
    }
   ],
   "source": [
    "# Test predictions\n",
    "print(\"=\" * 70)\n",
    "print(\"VALIDATION: Province Blend + LAD Model\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "test_lanes = [\n",
    "    ('Ø§Ù„Ø±ÙŠØ§Ø¶', 'Ø¬Ø¯Ø©'),\n",
    "    ('Ø§Ù„Ø¯Ù…Ø§Ù…', 'Ø§Ù„Ø±ÙŠØ§Ø¶'),\n",
    "    ('Ø¬Ø¯Ø©', 'Ø§Ù„Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„Ù…Ù†ÙˆØ±Ø©'),\n",
    "    ('ØªØ¨ÙˆÙƒ', 'Ø§Ù„Ø¯Ù…Ø§Ù…'),\n",
    "    ('Ø§Ù„Ø¬Ø¨ÙŠÙ„', 'Ø§Ù„Ø±ÙŠØ§Ø¶'),\n",
    "]\n",
    "\n",
    "for pickup, dest in test_lanes:\n",
    "    result = predict_blend_calibrated(pickup, dest)\n",
    "    if result:\n",
    "        print(f\"\\n{pickup} â†’ {dest}:\")\n",
    "        print(f\"   Raw: {result['raw_cpk']:.3f} â†’ Calibrated: {result['predicted_cpk']:.3f} SAR/km\")\n",
    "        print(f\"   Method: {result['method']}\")\n",
    "        if result['province_cpk']:\n",
    "            print(f\"   Province ({result['pickup_province']}â†’{result['dest_province']}): {result['province_cpk']:.3f}\")\n",
    "        if result['city_decomp_cpk']:\n",
    "            print(f\"   City decomp: {result['city_decomp_cpk']:.3f}\")\n",
    "    else:\n",
    "        print(f\"\\n{pickup} â†’ {dest}: No prediction (missing data)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "LEAVE-ONE-OUT VALIDATION: Simulating New Lanes\n",
      "======================================================================\n",
      "\n",
      "Tested 50 sparse lanes (â‰¤5 trips):\n",
      "   MAE:  0.5231 SAR/km\n",
      "   Bias: -0.2366 SAR/km\n",
      "   Within Â±0.25: 40.0%\n",
      "   Within Â±0.50: 64.0%\n"
     ]
    }
   ],
   "source": [
    "# Leave-one-out validation for NEW lane scenario\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"LEAVE-ONE-OUT VALIDATION: Simulating New Lanes\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# For each lane with few trips, pretend it's \"new\" and predict\n",
    "lane_trip_counts = df.groupby('lane').size()\n",
    "sparse_lanes = lane_trip_counts[lane_trip_counts <= 5].index.tolist()[:50]  # Test first 50\n",
    "\n",
    "loo_results = []\n",
    "for lane in sparse_lanes:\n",
    "    pickup, dest = lane.split(' â†’ ')\n",
    "    actual_cpk = df[df['lane'] == lane]['cost_per_km'].median()\n",
    "    \n",
    "    result = predict_blend_calibrated(pickup, dest)\n",
    "    if result:\n",
    "        loo_results.append({\n",
    "            'lane': lane,\n",
    "            'actual': actual_cpk,\n",
    "            'predicted': result['predicted_cpk'],\n",
    "            'residual': result['predicted_cpk'] - actual_cpk\n",
    "        })\n",
    "\n",
    "if loo_results:\n",
    "    loo_df = pd.DataFrame(loo_results)\n",
    "    print(f\"\\nTested {len(loo_df)} sparse lanes (â‰¤5 trips):\")\n",
    "    print(f\"   MAE:  {loo_df['residual'].abs().mean():.4f} SAR/km\")\n",
    "    print(f\"   Bias: {loo_df['residual'].mean():.4f} SAR/km\")\n",
    "    print(f\"   Within Â±0.25: {(loo_df['residual'].abs() <= 0.25).mean()*100:.1f}%\")\n",
    "    print(f\"   Within Â±0.50: {(loo_df['residual'].abs() <= 0.50).mean()*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# ğŸ’¾ EXPORT REFERENCE DATA & CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference Data Parameters:\n",
      "   Recency cutoff: 90 days\n"
     ]
    }
   ],
   "source": [
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘                    REFERENCE DATA PARAMETERS                         â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "RECENCY_CUTOFF_DAYS = 90    # Days to consider for recency model in app\n",
    "\n",
    "print(\"Reference Data Parameters:\")\n",
    "print(f\"   Recency cutoff: {RECENCY_CUTOFF_DAYS} days\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved: final_models/config.pkl\n",
      "   Distance lookup: 299 lanes\n",
      "   Backhaul lookup: 24 cities\n"
     ]
    }
   ],
   "source": [
    "# Create config.pkl\n",
    "DISTANCE_LOOKUP = df[df['distance'] > 0].groupby('lane')['distance'].median().to_dict()\n",
    "\n",
    "BACKHAUL_LOOKUP = {}\n",
    "if 'destination_city' in backhaul_df.columns and 'backhaul_probability_pct' in backhaul_df.columns:\n",
    "    BACKHAUL_LOOKUP = backhaul_df.set_index('destination_city')['backhaul_probability_pct'].to_dict()\n",
    "\n",
    "config = {\n",
    "    'FEATURES': ['entity_mapping', 'pickup_city', 'destination_city', 'commodity', \n",
    "                 'vehicle_type', 'distance', 'weight'],\n",
    "    'ENTITY_MAPPING': ENTITY_MAPPING,\n",
    "    'DISTANCE_LOOKUP': DISTANCE_LOOKUP,\n",
    "    'BACKHAUL_LOOKUP': BACKHAUL_LOOKUP,\n",
    "    'RECENCY_CUTOFF_DAYS': RECENCY_CUTOFF_DAYS,\n",
    "}\n",
    "\n",
    "with open(f'{EXPORT_DIR}/config.pkl', 'wb') as f:\n",
    "    pickle.dump(config, f)\n",
    "\n",
    "print(f\"âœ… Saved: {EXPORT_DIR}/config.pkl\")\n",
    "print(f\"   Distance lookup: {len(DISTANCE_LOOKUP)} lanes\")\n",
    "print(f\"   Backhaul lookup: {len(BACKHAUL_LOOKUP)} cities\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Saved: final_models/reference_data.csv\n",
      "   Rows: 84,948\n",
      "   Columns: 17\n"
     ]
    }
   ],
   "source": [
    "# Export reference_data.csv\n",
    "export_cols = [\n",
    "    'pickup_date', 'entity_mapping', 'commodity', 'vehicle_type',\n",
    "    'pickup_city', 'destination_city', 'distance', 'weight',\n",
    "    'total_carrier_price', 'days_ago', 'lane',\n",
    "    'container', 'is_multistop', 'dest_backhaul_prob', 'lane_imbalance', 'lane_load_count'\n",
    "]\n",
    "\n",
    "for col in export_cols:\n",
    "    if col not in df.columns:\n",
    "        df[col] = 0\n",
    "\n",
    "if 'total_shipper_price' in df.columns:\n",
    "    export_cols.insert(export_cols.index('days_ago'), 'total_shipper_price')\n",
    "\n",
    "df[export_cols].to_csv(f'{EXPORT_DIR}/reference_data.csv', index=False)\n",
    "\n",
    "print(f\"\\nâœ… Saved: {EXPORT_DIR}/reference_data.csv\")\n",
    "print(f\"   Rows: {len(df):,}\")\n",
    "print(f\"   Columns: {len(export_cols)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved: final_models/distance_matrix.pkl\n",
      "   City pairs: 4,492\n"
     ]
    }
   ],
   "source": [
    "# Export distance_matrix.pkl\n",
    "with open(f'{EXPORT_DIR}/distance_matrix.pkl', 'wb') as f:\n",
    "    pickle.dump(hardcoded_distances, f)\n",
    "\n",
    "print(f\"âœ… Saved: {EXPORT_DIR}/distance_matrix.pkl\")\n",
    "print(f\"   City pairs: {len(hardcoded_distances):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy city normalization file\n",
    "if os.path.exists(CITY_REGIONS_FILE):\n",
    "    shutil.copy(CITY_REGIONS_FILE, f'{EXPORT_DIR}/city_normalization_with_regions.csv')\n",
    "    print(f\"âœ… Copied: {EXPORT_DIR}/city_normalization_with_regions.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# ğŸ¤– MODEL 4: CATBOOST (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘                     CATBOOST PARAMETERS                              â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "TRAIN_CATBOOST = False       # Set True to train CatBoost model\n",
    "CATBOOST_ITERATIONS = 300\n",
    "CATBOOST_DEPTH = 9\n",
    "CATBOOST_LR = 0.05\n",
    "WEIGHT_HALF_LIFE_DAYS = 45   # Sample weighting decay\n",
    "\n",
    "print(\"CatBoost Parameters:\")\n",
    "print(f\"   Training: {'Enabled' if TRAIN_CATBOOST else 'Disabled'}\")\n",
    "if TRAIN_CATBOOST:\n",
    "    print(f\"   Iterations: {CATBOOST_ITERATIONS}\")\n",
    "    print(f\"   Depth: {CATBOOST_DEPTH}\")\n",
    "    print(f\"   Learning rate: {CATBOOST_LR}\")\n",
    "    print(f\"   Weight half-life: {WEIGHT_HALF_LIFE_DAYS} days\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_CATBOOST:\n",
    "    from catboost import CatBoostRegressor, Pool\n",
    "    from sklearn.metrics import mean_absolute_error, r2_score\n",
    "    \n",
    "    print(\"Training CatBoost model...\")\n",
    "    \n",
    "    FEATURES = ['entity_mapping', 'pickup_city', 'destination_city', \n",
    "                'commodity', 'vehicle_type', 'distance', 'weight']\n",
    "    CAT_FEATURES = ['entity_mapping', 'pickup_city', 'destination_city', \n",
    "                    'commodity', 'vehicle_type']\n",
    "    CAT_INDICES = [FEATURES.index(f) for f in CAT_FEATURES]\n",
    "    \n",
    "    # Time-based split\n",
    "    recent_df = df[df['days_ago'] <= 60].sort_values('days_ago')\n",
    "    n_test = int(len(recent_df) * 0.2)\n",
    "    \n",
    "    test_df = recent_df.head(n_test)\n",
    "    train_df = recent_df.iloc[n_test:].copy()\n",
    "    \n",
    "    train_df['sample_weight'] = np.exp(-np.log(2) * train_df['days_ago'] / WEIGHT_HALF_LIFE_DAYS)\n",
    "    \n",
    "    print(f\"   Training: {len(train_df):,} samples\")\n",
    "    print(f\"   Testing: {len(test_df):,} samples\")\n",
    "    \n",
    "    carrier_model = CatBoostRegressor(\n",
    "        iterations=CATBOOST_ITERATIONS,\n",
    "        depth=CATBOOST_DEPTH,\n",
    "        learning_rate=CATBOOST_LR,\n",
    "        random_seed=42,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    carrier_pool = Pool(\n",
    "        train_df[FEATURES],\n",
    "        train_df['total_carrier_price'],\n",
    "        cat_features=CAT_INDICES,\n",
    "        weight=train_df['sample_weight']\n",
    "    )\n",
    "    carrier_model.fit(carrier_pool)\n",
    "    \n",
    "    # Evaluate\n",
    "    test_pred = carrier_model.predict(test_df[FEATURES])\n",
    "    mae = mean_absolute_error(test_df['total_carrier_price'], test_pred)\n",
    "    r2 = r2_score(test_df['total_carrier_price'], test_pred)\n",
    "    errors = np.abs(test_pred - test_df['total_carrier_price']) / test_df['total_carrier_price']\n",
    "    \n",
    "    print(f\"\\nâœ… CatBoost Model Performance:\")\n",
    "    print(f\"   MAE: {mae:,.0f} SAR\")\n",
    "    print(f\"   RÂ²: {r2:.3f}\")\n",
    "    print(f\"   Within 10%: {(errors <= 0.10).mean()*100:.1f}%\")\n",
    "    print(f\"   Within 15%: {(errors <= 0.15).mean()*100:.1f}%\")\n",
    "    print(f\"   Within 20%: {(errors <= 0.20).mean()*100:.1f}%\")\n",
    "    \n",
    "    # Save\n",
    "    carrier_model.save_model(f'{EXPORT_DIR}/carrier_model.json', format='json')\n",
    "    print(f\"\\nâœ… Saved: {EXPORT_DIR}/carrier_model.json\")\n",
    "    \n",
    "    config['FEATURES'] = FEATURES\n",
    "    config['CAT_FEATURES'] = CAT_FEATURES\n",
    "    config['CAT_INDICES'] = CAT_INDICES\n",
    "    \n",
    "    with open(f'{EXPORT_DIR}/config.pkl', 'wb') as f:\n",
    "        pickle.dump(config, f)\n",
    "else:\n",
    "    print(\"â­ï¸ Skipping CatBoost training (TRAIN_CATBOOST = False)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# ğŸ“‹ EXPORT SUMMARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"ğŸ“¦ EXPORT COMPLETE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nğŸ“ Output directory: {os.path.abspath(EXPORT_DIR)}\")\n",
    "print(\"\\nFiles created:\")\n",
    "for f in sorted(os.listdir(EXPORT_DIR)):\n",
    "    if f.startswith('.'): continue\n",
    "    size = os.path.getsize(f'{EXPORT_DIR}/{f}')\n",
    "    if size > 1024*1024:\n",
    "        size_str = f\"{size/1024/1024:.1f} MB\"\n",
    "    elif size > 1024:\n",
    "        size_str = f\"{size/1024:.1f} KB\"\n",
    "    else:\n",
    "        size_str = f\"{size} B\"\n",
    "    print(f\"   {f}: {size_str}\")\n",
    "\n",
    "print(f\"\\nğŸ“Š Training Data Summary:\")\n",
    "print(f\"   Trips: {len(df):,}\")\n",
    "print(f\"   Lanes: {df['lane'].nunique()}\")\n",
    "print(f\"   Date range: {df['date'].min()} to {df['date'].max()}\")\n",
    "print(f\"   Entity: {ENTITY_MAPPING}\")\n",
    "print(f\"   Vehicles: {VEHICLE_TYPES}\")\n",
    "\n",
    "print(f\"\\nğŸ¯ Model Components:\")\n",
    "print(f\"   Market index: {current_index:.4f} SAR/km\")\n",
    "print(f\"   Lane multipliers: {len(lane_multipliers)}\")\n",
    "print(f\"   Province corridors: {len(province_cpk)}\")\n",
    "print(f\"   5-Region corridors: {len(regional_cpk_5)}\")\n",
    "print(f\"   City-province mappings: {len(city_to_province)}\")\n",
    "print(f\"   Distance pairs: {len(historical_distances)} historical + {len(hardcoded_distances)} hardcoded\")\n",
    "\n",
    "print(f\"\\nğŸ“ LAD Calibration:\")\n",
    "print(f\"   Formula: P_calibrated = {lad_slope:.4f} Ã— P_raw + {lad_intercept:.4f}\")\n",
    "print(f\"   MAE improvement: {(raw_mae-cal_mae)/raw_mae*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final verification\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ğŸ§ª FINAL VERIFICATION - Loading exported models\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "with open(f'{EXPORT_DIR}/rare_lane_models.pkl', 'rb') as f:\n",
    "    loaded_rare = pickle.load(f)\n",
    "print(f\"\\nâœ… rare_lane_models.pkl loaded\")\n",
    "print(f\"   Lane stats: {len(loaded_rare['lane_stats'])}\")\n",
    "print(f\"   Current index: {loaded_rare['current_index']:.4f}\")\n",
    "\n",
    "with open(f'{EXPORT_DIR}/new_lane_model_blend.pkl', 'rb') as f:\n",
    "    loaded_blend = pickle.load(f)\n",
    "print(f\"\\nâœ… new_lane_model_blend.pkl loaded\")\n",
    "print(f\"   Model type: {loaded_blend['config'].get('model_type', 'Blend')}\")\n",
    "print(f\"   Province weight: {loaded_blend['config']['province_weight']}\")\n",
    "print(f\"   LAD slope: {loaded_blend['lad_slope']:.4f}\")\n",
    "print(f\"   LAD intercept: {loaded_blend['lad_intercept']:.4f}\")\n",
    "\n",
    "with open(f'{EXPORT_DIR}/config.pkl', 'rb') as f:\n",
    "    loaded_config = pickle.load(f)\n",
    "print(f\"\\nâœ… config.pkl loaded\")\n",
    "print(f\"   Distance lookup: {len(loaded_config['DISTANCE_LOOKUP'])} lanes\")\n",
    "\n",
    "loaded_ref = pd.read_csv(f'{EXPORT_DIR}/reference_data.csv')\n",
    "print(f\"\\nâœ… reference_data.csv loaded\")\n",
    "print(f\"   Rows: {len(loaded_ref):,}\")\n",
    "\n",
    "with open(f'{EXPORT_DIR}/distance_matrix.pkl', 'rb') as f:\n",
    "    loaded_dist = pickle.load(f)\n",
    "print(f\"\\nâœ… distance_matrix.pkl loaded\")\n",
    "print(f\"   City pairs: {len(loaded_dist):,}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"âœ… ALL MODELS VERIFIED SUCCESSFULLY\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘                    DEPLOYMENT INSTRUCTIONS                          â•‘\n",
    "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
    "â•‘                                                                      â•‘\n",
    "â•‘  1. Upload contents of final_models/ to GitHub:                      â•‘\n",
    "â•‘     â†’ pricing_app/model_export/                                      â•‘\n",
    "â•‘                                                                      â•‘\n",
    "â•‘  2. Files to upload:                                                 â•‘\n",
    "â•‘     - config.pkl                                                     â•‘\n",
    "â•‘     - reference_data.csv                                             â•‘\n",
    "â•‘     - rare_lane_models.pkl                                           â•‘\n",
    "â•‘     - new_lane_model_blend.pkl (Province Blend 0.7 + LAD)            â•‘\n",
    "â•‘     - distance_matrix.pkl                                            â•‘\n",
    "â•‘     - city_normalization_with_regions.csv                            â•‘\n",
    "â•‘     - carrier_model.json (if CatBoost trained)                       â•‘\n",
    "â•‘                                                                      â•‘\n",
    "â•‘  3. Streamlit Cloud will auto-deploy on push                         â•‘\n",
    "â•‘                                                                      â•‘\n",
    "â•‘  4. Pricing Cascade (in app):                                        â•‘\n",
    "â•‘     â€¢ Priority 1: Recency (median of last 90 days)                   â•‘\n",
    "â•‘     â€¢ Priority 2: Index + Shrinkage (lanes WITH history)             â•‘\n",
    "â•‘     â€¢ Priority 3: Province Blend 0.7 + LAD (NEW lanes)               â•‘\n",
    "â•‘                                                                      â•‘\n",
    "â•‘  5. Province Blend + LAD Calibration:                                â•‘\n",
    "â•‘     raw = 0.7 Ã— Province_CPK + 0.3 Ã— City_Decomposition              â•‘\n",
    "â•‘     calibrated = {:.4f} Ã— raw + {:.4f}                               â•‘\n",
    "â•‘                                                                      â•‘\n",
    "â•‘  6. Buy Price rounded to nearest 100 SAR                             â•‘\n",
    "â•‘     Sell Price = Buy Ã— (1 + margin) rounded to nearest 50 SAR        â•‘\n",
    "â•‘     Margin based on backhaul probability (12-25%)                    â•‘\n",
    "â•‘                                                                      â•‘\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\"\"\".format(lad_slope, lad_intercept))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "province                   region  \n",
       "Al Bahah Province          Southern     8\n",
       "Al Jouf Province           Northern     4\n",
       "Asir Province              Southern    21\n",
       "Eastern Province           Central      2\n",
       "                           Eastern     25\n",
       "Hail Province              Northern     6\n",
       "Jazan Province             Southern    11\n",
       "Madinah Province           Northern     1\n",
       "                           Western      9\n",
       "Makkah Province            Western     30\n",
       "Najran Province            Southern     6\n",
       "Northern Borders Province  Northern     3\n",
       "Qassim Province            Central      2\n",
       "                           Northern    15\n",
       "Riyadh Province            Central     24\n",
       "                           Eastern      1\n",
       "Tabuk Province             Northern     5\n",
       "                           Western      3\n",
       "Name: canonical, dtype: int64"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "city_regions_df.groupby(['province','region']).canonical.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Makkah Province': 'Western', 'Eastern Province': 'Eastern', 'Riyadh Province': 'Central', 'Asir Province': 'Southern', 'Qassim Province': 'Northern', 'Jazan Province': 'Southern', 'Madinah Province': 'Western', 'Al Bahah Province': 'Southern', 'Najran Province': 'Southern', 'Hail Province': 'Northern', 'Tabuk Province': 'Northern', 'Al Jouf Province': 'Northern', 'Northern Borders Province': 'Northern'}\n"
     ]
    }
   ],
   "source": [
    "# 1. Count unique cities per province/region pair\n",
    "region_counts = city_regions_df.groupby(['province', 'region'])['canonical'].nunique().reset_index(name='city_count')\n",
    "\n",
    "# 2. Sort by count (descending) so the most popular region is first\n",
    "region_counts = region_counts.sort_values('city_count', ascending=False)\n",
    "\n",
    "# 3. Drop duplicates on 'province' (keeping the first, which is the highest count)\n",
    "# This creates a 1-to-1 mapping of Province -> Dominant Region\n",
    "dominant_regions = region_counts.drop_duplicates('province', keep='first')\n",
    "\n",
    "# 4. Convert to a dictionary for easy mapping\n",
    "province_to_region_map = dict(zip(dominant_regions['province'], dominant_regions['region']))\n",
    "\n",
    "# View the result\n",
    "print(province_to_region_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
